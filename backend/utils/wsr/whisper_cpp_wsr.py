"""
whisper.cpp wrapper - Drop-in replacement for fast_wsr.py
Uses official whisper.cpp binary via subprocess
Supports CPU, CUDA, and Vulkan GPU acceleration
Single-threaded for better accuracy and context preservation
Real-time progress tracking via SRT timestamp parsing
"""
import subprocess
import json
import os
import time
import random
from typing import Callable, Optional, Dict, Any
from pathlib import Path

# Import progress tracking utilities
from .whisper_cpp_progress import (
    estimate_audio_duration,
    track_whisper_progress
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration & Model Management
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_configured_model_name() -> str:
    try:
        from video.views.set_setting import load_all_settings
        settings_data = load_all_settings()
        fwsr_model = settings_data.get('Transcription Engine', {}).get('fwsr_model', 'large-v3')

        # æ˜ å°„faster-whisperæ¨¡å‹ååˆ°whisper.cpp GGMLæ¨¡å‹
        model_mapping = {
            'large-v3': 'ggml-large-v3.bin',
            'large-v2': 'ggml-large-v2.bin',
            'medium': 'ggml-medium.bin',
            'small': 'ggml-small.bin',
            'base': 'ggml-base.bin',
            'tiny': 'ggml-tiny.bin',
            'large-v3-q5': 'ggml-large-v3-q5_0.bin',
            'medium-q5': 'ggml-medium-q5_0.bin',
        }
        return model_mapping.get(fwsr_model, 'ggml-large-v3.bin')
    except:
        return 'ggml-large-v3.bin'


def get_use_gpu_setting() -> bool:
    try:
        from video.views.set_setting import load_all_settings
        settings_data = load_all_settings()
        use_gpu_str = settings_data.get('Transcription Engine', {}).get('use_gpu', 'true')
        print(f"[whisper.cpp] use_gpu setting: {use_gpu_str}")
        return use_gpu_str.lower() in ('true', '1', 'yes')
    except:
        return True  # é»˜è®¤å¯ç”¨GPU


def _check_gpu_support(binary_path: Path) -> tuple[bool, str]:
    """
    æ£€æŸ¥whisper.cppäºŒè¿›åˆ¶çš„GPUæ”¯æŒç±»å‹
    è¿”å›: (æ”¯æŒGPU, GPUç±»å‹)  GPUç±»å‹å¯ä»¥æ˜¯ 'cuda', 'vulkan', æˆ– 'none'
    """
    try:
        # ä»æ–‡ä»¶ååˆ¤æ–­GPUç±»å‹
        binary_name = binary_path.name.lower()
        if 'vulkan' in binary_name:
            return (True, 'vulkan')
        elif 'cuda' in binary_name:
            return (True, 'cuda')
        elif 'cpu' in binary_name:
            return (False, 'none')

        # é€šè¿‡lddæ£€æŸ¥é“¾æ¥åº“
        env = os.environ.copy()
        source_dir = binary_path.parent / "source"
        lib_paths = [
            str(source_dir / "build" / "ggml" / "src" / "ggml-vulkan"),
            str(source_dir / "build" / "ggml" / "src" / "ggml-cuda"),
            str(source_dir / "build" / "ggml" / "src"),
            str(source_dir / "build" / "src"),
            "/usr/local/cuda-12.2/lib64",
        ]
        if "LD_LIBRARY_PATH" in env:
            lib_paths.append(env["LD_LIBRARY_PATH"])
        env["LD_LIBRARY_PATH"] = ":".join(lib_paths)

        result = subprocess.run(
            ['ldd', str(binary_path)],
            capture_output=True,
            text=True,
            timeout=5,
            env=env
        )
        output = result.stdout.lower()

        # æ£€æŸ¥ Vulkan æ”¯æŒ
        if 'libvulkan' in output or 'ggml-vulkan' in output:
            return (True, 'vulkan')
        # æ£€æŸ¥ CUDA æ”¯æŒ
        if 'libcuda' in output or 'libcublas' in output or 'ggml-cuda' in output:
            return (True, 'cuda')

        return (False, 'none')
    except:
        return (False, 'none')


def get_whisper_cpp_paths(use_gpu: bool = None) -> Dict[str, str]:
    """
    è·å–whisper.cppäºŒè¿›åˆ¶å’Œæ¨¡å‹è·¯å¾„
    æ ¹æ®use_gpuè®¾ç½®è‡ªåŠ¨é€‰æ‹©åˆé€‚ç‰ˆæœ¬

    Args:
        use_gpu: Trueä½¿ç”¨GPUç‰ˆæœ¬, Falseä½¿ç”¨CPUç‰ˆæœ¬, Noneè‡ªåŠ¨æ£€æµ‹é…ç½®

    è¿”å›: {"binary": "path/to/main", "model_dir": "path/to/models", "has_cuda": bool, "gpu_type": str}
    """
    if use_gpu is None:
        use_gpu = get_use_gpu_setting()

    current_dir = Path(__file__).resolve().parent
    project_root = current_dir.parent.parent
    whisper_cpp_dir = project_root / "bin" / "whisper-cpp"

    # å€™é€‰äºŒè¿›åˆ¶åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    candidate_bins = []
    if use_gpu:
        # GPUæ¨¡å¼ï¼šä¼˜å…ˆVulkan, ç„¶åCUDA
        candidate_bins.extend([
            whisper_cpp_dir / "main-vulkan",
            whisper_cpp_dir / "main-cuda",
            whisper_cpp_dir / "vulkan" / "bin" / "whisper-cli",  # Vulkanæ„å»ºè¾“å‡º
        ])
    # CPUä½œä¸ºfallback
    candidate_bins.append(whisper_cpp_dir / "main-cpu")

    # é€‰æ‹©äºŒè¿›åˆ¶
    selected_bin = None
    has_cuda = False
    gpu_type = 'none'

    for bin_path in candidate_bins:
        if not bin_path.exists():
            continue

        has_gpu, detected_gpu_type = _check_gpu_support(bin_path)

        if use_gpu and has_gpu:
            # GPUæ¨¡å¼ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªæ”¯æŒGPUçš„
            selected_bin = bin_path
            has_cuda = (detected_gpu_type == 'cuda')
            gpu_type = detected_gpu_type
            print(f"[whisper.cpp] âœ… é€‰ä¸­GPUç‰ˆæœ¬: {bin_path.name} (ç±»å‹: {detected_gpu_type.upper()})")
            break
        elif not use_gpu and not has_gpu:
            # CPUæ¨¡å¼ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªä¸æ”¯æŒGPUçš„
            selected_bin = bin_path
            gpu_type = 'cpu'
            print(f"[whisper.cpp] â„¹ï¸  é€‰ä¸­CPUç‰ˆæœ¬: {bin_path.name}")
            break
        elif selected_bin is None:
            # æš‚å­˜ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä½œä¸ºfallback
            selected_bin = bin_path
            has_cuda = (detected_gpu_type == 'cuda')
            gpu_type = detected_gpu_type

    # éªŒè¯æ‰¾åˆ°çš„äºŒè¿›åˆ¶
    if not selected_bin or not selected_bin.exists():
        raise FileNotFoundError(
            f"whisper.cpp binary not found!\n"
            f"Searched paths:\n" + "\n".join(f"  - {p}" for p in candidate_bins) +
            f"\n\nPlease:\n"
            f"1. Place binary at {whisper_cpp_dir}/main-cpu or main-vulkan or main-cuda\n"
            f"2. Or build from source: cd backend/bin/whisper-cpp && make\n"
            f"\nDownload from: https://github.com/ggml-org/whisper.cpp/releases"
        )

    if use_gpu and gpu_type == 'none':
        print(f"[whisper.cpp] âš ï¸  GPUå·²å¯ç”¨ä½†äºŒè¿›åˆ¶ä¸æ”¯æŒGPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")

    # æ¨¡å‹ç›®å½•
    model_dir = os.getenv("WHISPER_MODEL_DIR")
    if not model_dir:
        model_dir = project_root / "models"

    return {
        "binary": str(selected_bin),
        "model_dir": str(model_dir),
        "has_cuda": has_cuda,
        "gpu_type": gpu_type  # 'cuda', 'vulkan', 'cpu', 'none'
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Core Transcription Function
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def transcribe_audio(
    audio_file_path: str,
    progress_cb: Callable[[str], None],
    language: str = None
) -> str:
    """
    ä½¿ç”¨whisper.cppè½¬å½•éŸ³é¢‘æ–‡ä»¶ï¼Œç”Ÿæˆword-levelæ—¶é—´æˆ³çš„SRTå­—å¹•
    å•çº¿ç¨‹å¤„ç†ï¼Œä¿æŒå®Œæ•´ä¸Šä¸‹æ–‡å’Œæœ€ä½³å‡†ç¡®æ€§

    Args:
        audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        progress_cb: è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆæ¥æ”¶å­—ç¬¦ä¸²çŠ¶æ€ï¼‰
        language: è¯­è¨€ä»£ç  (zh/en/jp/Noneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹)

    Returns:
        SRTæ ¼å¼å­—å¹•å†…å®¹
    """
    progress_cb("Running")

    # è·å–GPUè®¾ç½®
    use_gpu = get_use_gpu_setting()

    # è·å–whisper.cppè·¯å¾„å’Œæ¨¡å‹
    paths = get_whisper_cpp_paths(use_gpu=use_gpu)
    binary_path = paths["binary"]
    model_dir = Path(paths["model_dir"])
    gpu_type = paths.get("gpu_type", "none")
    model_name = get_configured_model_name()
    model_path = model_dir / model_name

    # è½¬æ¢éŸ³é¢‘æ–‡ä»¶ä¸ºç»å¯¹è·¯å¾„
    audio_file_path_abs = str(Path(audio_file_path).resolve())

    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not Path(binary_path).exists():
        raise FileNotFoundError(
            f"whisper.cpp binary not found at {binary_path}\n"
            f"Please install whisper.cpp binary\n"
            f"Download from: https://github.com/ggml-org/whisper.cpp/releases"
        )

    if not model_path.exists():
        raise FileNotFoundError(
            f"Model not found at {model_path}\n"
            f"Please download models using: bash backend/scripts/download_whisper_models.sh\n"
            f"Or manually from: https://huggingface.co/ggerganov/whisper.cpp/tree/main"
        )

    if not Path(audio_file_path_abs).exists():
        raise FileNotFoundError(
            f"Audio file not found at {audio_file_path_abs}\n"
            f"Original path: {audio_file_path}"
        )

    gpu_type_display = gpu_type.upper()
    print(f"[whisper.cpp] Binary: {binary_path} (GPU: {gpu_type_display})")
    print(f"[whisper.cpp] Model: {model_path}")
    print(f"[whisper.cpp] Audio: {audio_file_path_abs}")

    if use_gpu and gpu_type == 'cuda':
        print(f"[whisper.cpp] Device: ğŸš€ GPU (CUDA)")
    elif use_gpu and gpu_type == 'vulkan':
        print(f"[whisper.cpp] Device: ğŸŒ‹ GPU (Vulkan)")
    else:
        print(f"[whisper.cpp] Device: ğŸŒ CPU-only")

    # æ„å»ºwhisper.cppå‘½ä»¤
    cmd = [
        str(binary_path),
        "-m", str(model_path),
        "-f", audio_file_path_abs,
        "-ojf",  # JSONè¾“å‡ºæ ¼å¼ï¼ŒåŒ…å«word-level timestamps
        "-fa",   # å¼ºåˆ¶éŸ³é¢‘å¤„ç†
        "-ml", "3",  # æœ€å¤§è¡Œé•¿åº¦
        "--dtw", "large.v3",  # åŠ¨æ€æ—¶é—´è§„æ•´
        "-t", "8",   # 8çº¿ç¨‹ï¼ˆCPUæ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰
        "-bs", "5",  # beam_size=5
        "-bo", "5",  # best_of=5
    ]

    # GPU/CPUæ§åˆ¶ - å…³é”®ä¿®å¤
    if not use_gpu or gpu_type == 'none':
        # æ˜ç¡®ç¦ç”¨GPU
        cmd.extend(["-ng"])
        print(f"[whisper.cpp] å·²é€šè¿‡ -ng å‚æ•°ç¦ç”¨GPU")
    else:
        # GPUæ¨¡å¼ï¼šwhisper.cppä¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨GPU
        print(f"[whisper.cpp] GPUæ¨¡å¼å¯ç”¨ ({gpu_type})")

    # è¯­è¨€å‚æ•°
    if language and language != "None":
        cmd.extend(["-l", language])
        print(f"[whisper.cpp] Language: {language}")
    else:
        print(f"[whisper.cpp] Auto-detecting language")

    print(f"[whisper.cpp] Command: {' '.join(cmd)}")

    # è·å–éŸ³é¢‘æ—¶é•¿ç”¨äºè¿›åº¦è®¡ç®—
    print(f"[whisper.cpp] Detecting audio duration...")
    total_duration = estimate_audio_duration(audio_file_path_abs)
    print(f"[whisper.cpp] Total duration: {total_duration:.1f}s ({total_duration/60:.1f} min)")

    # æ‰§è¡Œwhisper.cpp
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡æ”¯æŒGPUåº“
        env = os.environ.copy()
        binary_dir = Path(binary_path).parent

        # æ ¹æ®GPUç±»å‹è®¾ç½®åº“è·¯å¾„
        lib_paths = []

        if gpu_type == 'vulkan':
            # Vulkanåº“è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
            vulkan_lib_dirs = [
                binary_dir / "vulkan" / "lib",  # æ ‡å‡†Vulkanæ„å»ºè¾“å‡º
                binary_dir / "lib",  # å¤‡é€‰libç›®å½•
            ]
            for lib_dir in vulkan_lib_dirs:
                if lib_dir.exists():
                    lib_paths.append(str(lib_dir))
                    print(f"[whisper.cpp] æ·»åŠ Vulkanåº“è·¯å¾„: {lib_dir}")

        elif gpu_type == 'cuda':
            # CUDAåº“è·¯å¾„
            cuda_lib_dirs = [
                "/usr/local/cuda-12.2/lib64",
                "/usr/local/cuda/lib64",
                binary_dir / "cuda" / "lib",
            ]
            for lib_dir in cuda_lib_dirs:
                if Path(lib_dir).exists():
                    lib_paths.append(str(lib_dir))
                    print(f"[whisper.cpp] æ·»åŠ CUDAåº“è·¯å¾„: {lib_dir}")

        # æ·»åŠ æºç æ„å»ºç›®å½•ï¼ˆå¯é€‰ï¼‰
        source_dir = binary_dir / "source"
        if source_dir.exists():
            source_lib_paths = [
                source_dir / "build" / "ggml" / "src",
                source_dir / "build" / "ggml" / "src" / "ggml-cuda",
                source_dir / "build" / "ggml" / "src" / "ggml-vulkan",
            ]
            for lib_dir in source_lib_paths:
                if lib_dir.exists():
                    lib_paths.append(str(lib_dir))

        # ä¿ç•™åŸæœ‰çš„LD_LIBRARY_PATH
        if "LD_LIBRARY_PATH" in env:
            lib_paths.append(env["LD_LIBRARY_PATH"])

        # è®¾ç½®ç¯å¢ƒå˜é‡
        if lib_paths:
            env["LD_LIBRARY_PATH"] = ":".join(lib_paths)
            print(f"[whisper.cpp] LD_LIBRARY_PATH: {env['LD_LIBRARY_PATH']}")
        else:
            print(f"[whisper.cpp] ä½¿ç”¨ç³»ç»Ÿé»˜è®¤LD_LIBRARY_PATH")

        # å¯åŠ¨whisper.cppè¿›ç¨‹ (ä½¿ç”¨Popenè·å–å®æ—¶è¾“å‡º)
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding='utf-8',
            cwd=str(Path(binary_path).parent),
            env=env
        )

        # å®šä¹‰è¿›åº¦å›è°ƒåŒ…è£…å™¨
        def on_whisper_progress(percent: int, detail: str):
            """whisper.cppè¿›åº¦å›è°ƒ: è§£ææ—¶é—´æˆ³åçš„ç™¾åˆ†æ¯”"""
            # å°†ç™¾åˆ†æ¯”ä¼ é€’ç»™å¤–éƒ¨å›è°ƒ
            progress_cb(percent)

        # å®æ—¶è¿½è¸ªè¿›åº¦å¹¶è·å–stdoutè¾“å‡º
        print(f"[whisper.cpp] Starting real-time progress tracking...")
        stdout_str = track_whisper_progress(
            process=process,
            total_duration=total_duration,
            callback=on_whisper_progress,
            encoding='utf-8'
        )

        # è¯»å–stderr
        stderr_str = process.stderr.read()

        print(f"[whisper.cpp] âœ… Transcription completed")

        # whisper.cpp çš„ -oj å‚æ•°ä¼šå°† JSON å†™å…¥åˆ° <audio_file>.json æ–‡ä»¶
        json_file_path = Path(audio_file_path_abs + ".json")

        if not json_file_path.exists():
            raise FileNotFoundError(
                f"whisper.cpp did not create JSON output file: {json_file_path}\n"
                f"stderr: {stderr_str[:200]}"
            )

        # åˆ›å»º work_dir ç›®å½•ï¼ˆä¿å­˜JSONå¤‡ä»½ï¼‰
        work_dir = Path(__file__).resolve().parent.parent.parent / "work_dir"
        work_dir.mkdir(exist_ok=True)

        # è¯»å–JSONæ–‡ä»¶ (whisper.cpp outputs UTF-8 text, not binary)
        # ä½¿ç”¨ errors='replace' å¤„ç† -ml å‚æ•°å¯¼è‡´çš„UTF-8æˆªæ–­é—®é¢˜
        # æ— æ•ˆçš„UTF-8å­—èŠ‚ä¼šè¢«æ›¿æ¢ä¸º ï¿½ (U+FFFD)ï¼Œä½†é¡¶å±‚textå­—æ®µä»ç„¶æ­£ç¡®
        with open(json_file_path, 'r', encoding='utf-8', errors='replace') as f:
            json_str = f.read()

        # ä¿å­˜å¤‡ä»½
        timestamp = int(time.time() * 1000)
        random_suffix = random.randint(1000, 9999)
        saved_json_filename = f"whisper_cpp_{timestamp}_{random_suffix}.json"
        saved_json_path = work_dir / saved_json_filename
        with open(saved_json_path, 'w', encoding='utf-8') as f:
            f.write(json_str)
        print(f"[whisper.cpp] âœ… JSONå¤‡ä»½å·²ä¿å­˜: {saved_json_path}")

        # è§£æJSON
        transcription_data = json.loads(json_str)
        # print(f"[whisper.cpp] âœ… JSONè§£ææˆåŠŸ, transcriptionæ¡ç›®æ•°: {len(transcription_data.get('transcription', []))}")

        srt_content = _convert_whisper_cpp_to_srt(transcription_data)
        # print(f"[whisper.cpp] æˆåŠŸè½¬æ¢ä¸ºSRT, å­—å¹•æ¡ç›®æ•°: {srt_content.count('-->')}")

        # Debug: Check first 200 chars of SRT content

        # åˆ é™¤whisper.cppç”Ÿæˆçš„ä¸´æ—¶JSONæ–‡ä»¶
        try:
            json_file_path.unlink()
        except Exception as e:
            print(f"[whisper.cpp] æ— æ³•åˆ é™¤ä¸´æ—¶JSON: {e}")

        progress_cb("Completed")
        return srt_content

    except subprocess.CalledProcessError as e:
        stderr_msg = e.stderr if isinstance(e.stderr, str) else str(e.stderr)
        print(f"[whisper.cpp] Error: {stderr_msg}")
        raise RuntimeError(f"whisper.cpp transcription failed: {stderr_msg}")
    except json.JSONDecodeError as e:
        print(f"[whisper.cpp] JSON parsing error: {e}")
        raise RuntimeError(f"Failed to parse whisper.cpp output: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _convert_whisper_cpp_to_srt(json_data: Dict[str, Any]) -> str:
    """
    å°†whisper.cppçš„JSONè¾“å‡ºè½¬æ¢ä¸ºSRTå­—å¹•æ ¼å¼

    whisper.cpp JSONæ ¼å¼:
    {
      "transcription": [
        {
          "timestamps": {"from": "00:00:00,320", "to": "00:00:00,370"},
          "offsets": {"from": 320, "to": 370},
          "text": " word"
        },
        ...
      ]
    }
    """
    srt_lines = []
    index = 1

    transcription = json_data.get("transcription", [])

    for segment in transcription:
        # åªä½¿ç”¨é¡¶å±‚çš„textå­—æ®µï¼Œå¿½ç•¥tokensæ•°ç»„
        # tokensä¸­çš„textå¯èƒ½åŒ…å« ï¿½ ä¹±ç (ç”±äº -ml å‚æ•°æˆªæ–­UTF-8)
        # ä½†é¡¶å±‚textæ˜¯å®Œæ•´æ­£ç¡®çš„è¯†åˆ«ç»“æœ
        text = segment.get("text", "").strip()
        if not text:
            continue

        timestamps = segment.get("timestamps", {})
        time_from = timestamps.get("from", "00:00:00,000")
        time_to = timestamps.get("to", "00:00:00,000")

        # Write into subtitle format.
        srt_lines.append(f"{index}")
        srt_lines.append(f"{time_from} --> {time_to}")
        srt_lines.append(text)
        srt_lines.append("")

        index += 1

    return "\n".join(srt_lines)
