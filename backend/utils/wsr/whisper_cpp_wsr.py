"""
whisper.cpp wrapper - Drop-in replacement for fast_wsr.py
Uses official whisper.cpp binary via subprocess
Supports both CPU-only and CUDA GPU acceleration
"""
import subprocess
import json
import os
import shutil
import time
import random
from typing import Callable, Optional, Dict, Any
from pathlib import Path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Configuration & Model Management
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_configured_model_name() -> str:
    try:
        from video.views.set_setting import load_all_settings
        settings_data = load_all_settings()
        fwsr_model = settings_data.get('Transcription Engine', {}).get('fwsr_model', 'large-v3')

        # æ˜ å°„faster-whisperæ¨¡å‹ååˆ°whisper.cpp GGMLæ¨¡å‹
        model_mapping = {
            'large-v3': 'ggml-large-v3.bin',
            'large-v2': 'ggml-large-v2.bin',
            'medium': 'ggml-medium.bin',
            'small': 'ggml-small.bin',
            'base': 'ggml-base.bin',
            'tiny': 'ggml-tiny.bin',
            'large-v3-q5': 'ggml-large-v3-q5_0.bin',
            'medium-q5': 'ggml-medium-q5_0.bin',
        }
        return model_mapping.get(fwsr_model, 'ggml-large-v3.bin')
    except:
        return 'ggml-large-v3.bin'


def get_use_gpu_setting() -> bool:
    try:
        from video.views.set_setting import load_all_settings
        settings_data = load_all_settings()
        use_gpu_str = settings_data.get('Transcription Engine', {}).get('use_gpu', 'true')
        print(f"use_gpu_str:={use_gpu_str}")
        return use_gpu_str.lower() in ('true', '1', 'yes')
    except:
        return True  # é»˜è®¤å¯ç”¨GPU


def get_whisper_cpp_binary_preference() -> str:
    """
    è·å–whisper.cppäºŒè¿›åˆ¶ä¼˜å…ˆçº§é…ç½®
    è¿”å›: 'auto' (è‡ªåŠ¨é€‰æ‹©), 'cpu', 'cuda', 'vulkan', æˆ–å…·ä½“è·¯å¾„
    """
    try:
        from video.views.set_setting import load_all_settings
        settings_data = load_all_settings()
        return settings_data.get('Transcription Engine', {}).get('whisper_cpp_binary', 'auto')
    except:
        return 'auto'


def _check_gpu_support(binary_path: Path) -> tuple[bool, str]:
    """
    æ£€æŸ¥whisper.cppäºŒè¿›åˆ¶çš„GPUæ”¯æŒç±»å‹
    è¿”å›: (æ”¯æŒGPU, GPUç±»å‹)  GPUç±»å‹å¯ä»¥æ˜¯ 'cuda', 'vulkan', æˆ– 'none'
    """
    try:
        # Set up library path for ldd to find dependencies
        env = os.environ.copy()
        source_dir = binary_path.parent / "source"
        lib_paths = [
            str(source_dir / "build" / "ggml" / "src" / "ggml-vulkan"),
            str(source_dir / "build" / "ggml" / "src" / "ggml-cuda"),
            str(source_dir / "build" / "ggml" / "src"),
            str(source_dir / "build" / "src"),
            "/usr/local/cuda-12.2/lib64",
        ]
        if "LD_LIBRARY_PATH" in env:
            lib_paths.append(env["LD_LIBRARY_PATH"])
        env["LD_LIBRARY_PATH"] = ":".join(lib_paths)

        result = subprocess.run(
            ['ldd', str(binary_path)],
            capture_output=True,
            text=True,
            timeout=5,
            env=env
        )
        output = result.stdout.lower()
        print(f"[whisper.cpp] ldd output for {binary_path}:\n{output}")
        # æ£€æŸ¥ Vulkan æ”¯æŒ
        if 'libvulkan' in output or 'ggml-vulkan' in output:
            return (True, 'vulkan')

        # æ£€æŸ¥ CUDA æ”¯æŒ
        if 'libcuda' in output or 'libcublas' in output or 'ggml-cuda' in output:
            return (True, 'cuda')

        return (False, 'none')
    except:
        return (False, 'none')


def get_whisper_cpp_paths(use_gpu: bool = None) -> Dict[str, str]:
    """
    è·å–whisper.cppäºŒè¿›åˆ¶å’Œæ¨¡å‹è·¯å¾„
    æ ¹æ®use_gpuè®¾ç½®å’Œbinary_preferenceè‡ªåŠ¨é€‰æ‹©åˆé€‚ç‰ˆæœ¬

    Args:
        use_gpu: Trueä½¿ç”¨GPUç‰ˆæœ¬, Falseä½¿ç”¨CPUç‰ˆæœ¬, Noneè‡ªåŠ¨æ£€æµ‹é…ç½®

    è¿”å›: {"binary": "path/to/main-cpu", "model_dir": "path/to/models", "has_cuda": bool, "gpu_type": str}
    """
    if use_gpu is None:
        use_gpu = get_use_gpu_setting()


    current_dir = Path(__file__).resolve().parent
    project_root = current_dir.parent.parent
    whisper_cpp_dir = project_root / "bin" / "whisper-cpp"

    candidate_bins = []
    candidate_bins.extend([
        whisper_cpp_dir / "main-vulkan",
        whisper_cpp_dir / "main-cuda",
        whisper_cpp_dir / "main-cpu",
    ])

    # é€‰æ‹©äºŒè¿›åˆ¶
    selected_bin = None
    has_cuda = False
    gpu_type = 'none'

    for bin_path in candidate_bins:
        if not bin_path.exists():
            continue

        has_gpu, detected_gpu_type = _check_gpu_support(bin_path)

        if use_gpu:
            # GPUæ¨¡å¼ï¼šä¼˜å…ˆé€‰æ‹©æœ‰GPUæ”¯æŒçš„
            if has_gpu:
                selected_bin = bin_path
                has_cuda = (detected_gpu_type == 'cuda')
                gpu_type = detected_gpu_type
                print(f"[whisper.cpp] âœ… æ‰¾åˆ°GPUç‰ˆæœ¬: {bin_path} (ç±»å‹: {detected_gpu_type.upper()})")
                break
            elif selected_bin is None:
                # å¦‚æœæ²¡æ‰¾åˆ°GPUç‰ˆæœ¬ï¼Œæš‚å­˜ç¬¬ä¸€ä¸ªå¯ç”¨çš„
                selected_bin = bin_path
                gpu_type = detected_gpu_type
        else:
            # CPUæ¨¡å¼ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªå¯ç”¨çš„
            selected_bin = bin_path
            has_cuda = (detected_gpu_type == 'cuda')
            gpu_type = detected_gpu_type if detected_gpu_type != 'none' else 'cpu'
            print(f"[whisper.cpp] â„¹ï¸ ä½¿ç”¨äºŒè¿›åˆ¶: {bin_path} (æ£€æµ‹ç±»å‹: {detected_gpu_type}, æ¨¡å¼: CPU-only)")
            break

    # éªŒè¯æ‰¾åˆ°çš„äºŒè¿›åˆ¶
    if not selected_bin or not selected_bin.exists():
        raise FileNotFoundError(
            f"whisper.cpp binary not found!\n"
            f"Searched paths:\n" + "\n".join(f"  - {p}" for p in candidate_bins) +
            f"\n\nPlease:\n"
            f"1. Set WHISPER_CPP_BIN environment variable, OR\n"
            f"2. Place binary at {whisper_cpp_dir}/main-cpu or main-vulkan or main-cuda, OR\n"
            f"3. Set 'whisper_cpp_binary' in config.ini [Transcription Engine] section\n"
            f"\nDownload from: https://github.com/ggml-org/whisper.cpp/releases"
        )

    if use_gpu and gpu_type == 'none':
        print(f"[whisper.cpp] âš ï¸ GPUå·²å¯ç”¨ä½†äºŒè¿›åˆ¶ä¸æ”¯æŒGPUï¼Œå°†å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")

    # æ¨¡å‹ç›®å½•
    model_dir = os.getenv("WHISPER_MODEL_DIR")
    if not model_dir:
        model_dir = project_root / "models"

    return {
        "binary": str(selected_bin),
        "model_dir": str(model_dir),
        "has_cuda": has_cuda,
        "gpu_type": gpu_type  # 'cuda', 'vulkan', 'cpu', 'none'
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Core Transcription Function
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def transcribe_audio(
    audio_file_path: str,
    progress_cb: Callable[[float], None],
    language: str = None
) -> str:
    """
    ä½¿ç”¨whisper.cppè½¬å½•éŸ³é¢‘æ–‡ä»¶ï¼Œç”Ÿæˆword-levelæ—¶é—´æˆ³çš„SRTå­—å¹•

    Args:
        audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        progress_cb: è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆæ¥æ”¶0-100çš„è¿›åº¦æˆ–"Running"/"Completed"ï¼‰
        language: è¯­è¨€ä»£ç  (zh/en/jp/Noneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹)

    Returns:
        SRTæ ¼å¼å­—å¹•å†…å®¹
    """
    progress_cb("Running")

    # è·å–GPUè®¾ç½®
    use_gpu = get_use_gpu_setting()

    # è·å–whisper.cppè·¯å¾„å’Œæ¨¡å‹ï¼ˆæ ¹æ®use_gpuè‡ªåŠ¨é€‰æ‹©CUDA/CPUç‰ˆæœ¬ï¼‰
    paths = get_whisper_cpp_paths(use_gpu=use_gpu)
    binary_path = paths["binary"]
    model_dir = Path(paths["model_dir"])
    has_cuda = paths["has_cuda"]
    model_name = get_configured_model_name()
    model_path = model_dir / model_name

    # å¦‚æœå¯ç”¨GPUä½†æ²¡æœ‰ä»»ä½•GPUæ”¯æŒï¼ˆCUDAæˆ–Vulkanï¼‰ï¼Œå¼ºåˆ¶ä½¿ç”¨CPU
    gpu_type = paths.get("gpu_type", "none")
    if use_gpu and gpu_type == "none":
        print(f"[whisper.cpp] âš ï¸ GPUå·²å¯ç”¨ä½†äºŒè¿›åˆ¶ä¸æ”¯æŒGPUï¼Œå¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
        use_gpu = False

    # è½¬æ¢éŸ³é¢‘æ–‡ä»¶ä¸ºç»å¯¹è·¯å¾„
    audio_file_path_abs = str(Path(audio_file_path).resolve())

    # éªŒè¯æ–‡ä»¶å­˜åœ¨
    if not Path(binary_path).exists():
        raise FileNotFoundError(
            f"whisper.cpp binary not found at {binary_path}\n"
            f"Please set WHISPER_CPP_BIN environment variable or install to backend/bin/whisper-cpp/\n"
            f"Download from: https://github.com/ggml-org/whisper.cpp/releases"
        )

    if not model_path.exists():
        raise FileNotFoundError(
            f"Model not found at {model_path}\n"
            f"Please download models using: bash backend/scripts/download_whisper_models.sh\n"
            f"Or manually from: https://huggingface.co/ggerganov/whisper.cpp/tree/main"
        )

    if not Path(audio_file_path_abs).exists():
        raise FileNotFoundError(
            f"Audio file not found at {audio_file_path_abs}\n"
            f"Original path: {audio_file_path}"
        )

    gpu_type_display = paths.get("gpu_type", "none").upper()
    print(f"[whisper.cpp] Binary: {binary_path} (GPU: {gpu_type_display})")
    print(f"[whisper.cpp] Model: {model_path}")
    print(f"[whisper.cpp] Audio: {audio_file_path_abs}")
    if use_gpu and has_cuda:
        print(f"[whisper.cpp] Device: ğŸš€ GPU (CUDA)")
    elif use_gpu and paths.get("gpu_type") == "vulkan":
        print(f"[whisper.cpp] Device: ğŸŒ‹ GPU (Vulkan)")
    else:
        print(f"[whisper.cpp] Device: ğŸŒ CPU-only")

    # æ„å»ºwhisper.cppå‘½ä»¤
    cmd = [
        str(binary_path),
        "-m", str(model_path),
        "-f", audio_file_path_abs,  # ä½¿ç”¨ç»å¯¹è·¯å¾„
        "-ojf",  # JSONè¾“å‡ºæ ¼å¼ï¼ŒåŒ…å«word-level timestamps
        "-fa",
        "-ml","3",
        "--dtw","large.v3",
        "-l",language,
        "-t", "8",   # 8çº¿ç¨‹ï¼ˆCPUæ¨¡å¼ä¸‹ä½¿ç”¨ï¼ŒGPUæ¨¡å¼ä¸‹è‡ªåŠ¨è°ƒæ•´ï¼‰
        # ä¸ä½¿ç”¨ -ml å’Œ -sowï¼Œè®©whisper.cppè¾“å‡ºåŸå§‹word-level timestamps
        # åç»­åœ¨LLMä¼˜åŒ–é˜¶æ®µåˆå¹¶æˆå¥å­
    ]

    # GPU/CPUæ§åˆ¶
    if not use_gpu:
        # ç¦ç”¨GPUï¼Œå¼ºåˆ¶ä½¿ç”¨CPU
        cmd.extend(["-ng"])  # --no-gpu
        print(f"[whisper.cpp] GPU disabled via -ng flag")
    else:
        # GPUæ¨¡å¼ï¼šwhisper.cppä¼šè‡ªåŠ¨ä½¿ç”¨CUDAï¼ˆå¦‚æœç¼–è¯‘æ—¶å¯ç”¨ï¼‰
        # æ³¨æ„ï¼šéœ€è¦ä½¿ç”¨CUDAç¼–è¯‘çš„whisper.cppäºŒè¿›åˆ¶
        print(f"[whisper.cpp] GPU enabled (requires CUDA-compiled binary)")

    # è¯­è¨€å‚æ•°
    if language and language != "None":
        cmd.extend(["-l", language])
        print(f"[whisper.cpp] Using language: {language}")
    else:
        print(f"[whisper.cpp] Auto-detecting language")

    # å…¶ä»–ä¼˜åŒ–å‚æ•°ï¼ˆæ˜ å°„è‡ªfast_wsr.pyçš„å‚æ•°ï¼‰
    cmd.extend([
        "-bs", "5",        # beam_size=5
        "-bo", "5",        # best_of=5
        # "-tr",           # translate to English (å¯é€‰ï¼Œé»˜è®¤ä¸ç¿»è¯‘)
    ])

    print(f"[whisper.cpp] Command: {' '.join(cmd)}")

    # æ‰§è¡Œwhisper.cpp
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥æ”¯æŒCUDAåº“
        env = os.environ.copy()
        cuda_lib_path = "/usr/local/cuda-12.2/lib64"
        source_dir = Path(binary_path).parent / "source"
        ggml_lib_paths = [
            str(source_dir / "build" / "ggml" / "src"),
            str(source_dir / "build" / "ggml" / "src" / "ggml-cuda"),
        ]

        # æ„å»ºLD_LIBRARY_PATH
        lib_paths = [cuda_lib_path] + ggml_lib_paths
        if "LD_LIBRARY_PATH" in env:
            lib_paths.append(env["LD_LIBRARY_PATH"])
        env["LD_LIBRARY_PATH"] = ":".join(lib_paths)

        # ä¸ä½¿ç”¨ text=Trueï¼Œä»¥é¿å…è‡ªåŠ¨è§£ç é”™è¯¯
        # åç»­æ‰‹åŠ¨å¤„ç†å­—èŠ‚æµ
        result = subprocess.run(
            cmd,
            capture_output=True,
            check=False,  # å…ˆä¸æ£€æŸ¥è¿”å›ç ï¼Œæ‰‹åŠ¨å¤„ç†é”™è¯¯
            cwd=str(Path(binary_path).parent),  # åœ¨äºŒè¿›åˆ¶æ‰€åœ¨ç›®å½•æ‰§è¡Œ
            env=env
        )

        # æ‰‹åŠ¨è§£ç  stdout å’Œ stderrï¼Œä½¿ç”¨å¤šç§ç¼–ç å°è¯•
        stdout_str = ""
        stderr_str = ""

        for encoding in ['utf-8', 'gbk', 'latin-1']:
            try:
                stdout_str = result.stdout.decode(encoding)
                stderr_str = result.stderr.decode(encoding)
                print(f"[whisper.cpp] å‘½ä»¤è¾“å‡ºè§£ç æˆåŠŸ: {encoding}")
                break
            except UnicodeDecodeError:
                if encoding == 'latin-1':
                    # latin-1 åº”è¯¥æ€»æ˜¯æˆåŠŸ
                    stdout_str = result.stdout.decode('latin-1', errors='replace')
                    stderr_str = result.stderr.decode('latin-1', errors='replace')
                    print(f"[whisper.cpp] å‘½ä»¤è¾“å‡ºè§£ç ä½¿ç”¨ latin-1 (æœ‰æ›¿æ¢)")
                continue

        # æ£€æŸ¥è¿”å›ç 
        if result.returncode != 0:
            print(f"[whisper.cpp] âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥ (è¿”å›ç : {result.returncode})")
            print(f"[whisper.cpp] stdout: {stdout_str[:500]}")
            print(f"[whisper.cpp] stderr: {stderr_str[:500]}")
            raise subprocess.CalledProcessError(result.returncode, cmd, stdout_str, stderr_str)

        print(f"[whisper.cpp] âœ… Transcription completed (è¿”å›ç : {result.returncode})")
        if stdout_str:
            print(f"[whisper.cpp] stdoutå‰200å­—ç¬¦: {stdout_str[:200]}")
        if stderr_str:
            print(f"[whisper.cpp] stderrå‰200å­—ç¬¦: {stderr_str[:200]}")

        # whisper.cpp çš„ -oj å‚æ•°ä¼šå°† JSON å†™å…¥åˆ° <audio_file>.json æ–‡ä»¶
        json_file_path = Path(audio_file_path_abs + ".json")

        if not json_file_path.exists():
            raise FileNotFoundError(
                f"whisper.cpp did not create JSON output file: {json_file_path}\n"
                f"stdout: {result.stdout[:200]}\n"
                f"stderr: {result.stderr[:200]}"
            )

        # åˆ›å»º work_dir ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        work_dir = Path(__file__).resolve().parent.parent.parent / "work_dir"
        work_dir.mkdir(exist_ok=True)

        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³å’Œéšæœºæ•°çš„æ–‡ä»¶åä¿å­˜JSON
        timestamp = int(time.time() * 1000)  # æ¯«ç§’çº§æ—¶é—´æˆ³
        random_suffix = random.randint(1000, 9999)
        saved_json_filename = f"whisper_cpp_{timestamp}_{random_suffix}.json"
        saved_json_path = work_dir / saved_json_filename

        # è¯»å–JSONæ–‡ä»¶ - å…ˆè¯»å–åŸå§‹å­—èŠ‚,ç„¶åå°è¯•å¤šç§ç¼–ç 
        try:
            with open(json_file_path, 'rb') as f:
                raw_data = f.read()

            print(f"[whisper.cpp] JSONæ–‡ä»¶å¤§å°: {len(raw_data)} bytes")
            print(f"[whisper.cpp] åŸå§‹JSONæ–‡ä»¶è·¯å¾„: {json_file_path}")

            # ä¿å­˜åŸå§‹å­—èŠ‚åˆ° work_dir
            with open(saved_json_path, 'wb') as f:
                f.write(raw_data)
            print(f"[whisper.cpp] âœ… å·²ä¿å­˜JSONåˆ°: {saved_json_path}")

            # å°è¯•å¤šç§ç¼–ç æ–¹å¼è§£æ
            json_str = None
            encoding_used = None

            # 1. å°è¯•UTF-8
            try:
                json_str = raw_data.decode('utf-8')
                encoding_used = 'utf-8'
                print(f"[whisper.cpp] âœ… è§£ç æˆåŠŸ: UTF-8")
            except UnicodeDecodeError as e:
                print(f"[whisper.cpp] âš ï¸ UTF-8è§£ç å¤±è´¥: {e}")

                # 2. å°è¯•UTF-8-SIG (å¸¦BOM)
                try:
                    json_str = raw_data.decode('utf-8-sig')
                    encoding_used = 'utf-8-sig'
                    print(f"[whisper.cpp] âœ… è§£ç æˆåŠŸ: UTF-8-SIG")
                except UnicodeDecodeError as e2:
                    print(f"[whisper.cpp] âš ï¸ UTF-8-SIGè§£ç å¤±è´¥: {e2}")

                    # 3. å°è¯•GBK (ä¸­æ–‡Windowså¸¸ç”¨ç¼–ç )
                    try:
                        json_str = raw_data.decode('gbk')
                        encoding_used = 'gbk'
                        print(f"[whisper.cpp] âœ… è§£ç æˆåŠŸ: GBK")
                    except UnicodeDecodeError as e3:
                        print(f"[whisper.cpp] âš ï¸ GBKè§£ç å¤±è´¥: {e3}")

                        # 4. å°è¯•æ›¿æ¢é”™è¯¯å­—ç¬¦
                        try:
                            json_str = raw_data.decode('utf-8', errors='replace')
                            encoding_used = 'utf-8 (with errors replaced)'
                            print(f"[whisper.cpp] âš ï¸ UTF-8è§£ç (æ›¿æ¢é”™è¯¯): ä½¿ç”¨ï¿½æ›¿æ¢æ— æ•ˆå­—ç¬¦")
                        except Exception as e4:
                            print(f"[whisper.cpp] âš ï¸ UTF-8(replace)è§£ç å¤±è´¥: {e4}")

                            # 5. æœ€åä½¿ç”¨latin-1ä½œä¸ºå…œåº•
                            json_str = raw_data.decode('latin-1')
                            encoding_used = 'latin-1 (fallback)'
                            print(f"[whisper.cpp] âš ï¸ é™çº§åˆ°Latin-1è§£ç ")

            print(f"[whisper.cpp] æœ€ç»ˆä½¿ç”¨ç¼–ç : {encoding_used}")
            print(f"[whisper.cpp] JSONå­—ç¬¦ä¸²é•¿åº¦: {len(json_str)} chars")
            print(f"[whisper.cpp] JSONå‰200å­—ç¬¦: {json_str[:200]}")

            # è§£æJSON
            transcription_data = json.loads(json_str)
            print(f"[whisper.cpp] âœ… JSONè§£ææˆåŠŸ, transcriptionæ¡ç›®æ•°: {len(transcription_data.get('transcription', []))}")

        except json.JSONDecodeError as e:
            print(f"[whisper.cpp] âŒ JSONè§£æå¤±è´¥: {e}")
            print(f"[whisper.cpp] é”™è¯¯ä½ç½®: line={e.lineno}, column={e.colno}")
            print(f"[whisper.cpp] å·²ä¿å­˜çš„JSONæ–‡ä»¶: {saved_json_path}")
            raise RuntimeError(f"Failed to parse whisper.cpp JSON output: {e}\nJSONä¿å­˜åœ¨: {saved_json_path}")
        except Exception as e:
            print(f"[whisper.cpp] è¯»å–/è§£æJSONæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            print(f"[whisper.cpp] å·²ä¿å­˜çš„JSONæ–‡ä»¶: {saved_json_path}")
            raise RuntimeError(f"Failed to read/parse JSON output: {e}\nJSONä¿å­˜åœ¨: {saved_json_path}")

        # è½¬æ¢ä¸ºSRTæ ¼å¼
        srt_content = _convert_whisper_cpp_to_srt(transcription_data)
        print(f"[whisper.cpp] æˆåŠŸè½¬æ¢ä¸ºSRT, å­—å¹•æ¡ç›®æ•°: {srt_content.count('-->')}")

        # åˆ é™¤whisper.cppç”Ÿæˆçš„ä¸´æ—¶JSONæ–‡ä»¶
        try:
            json_file_path.unlink()
            print(f"[whisper.cpp] å·²åˆ é™¤ä¸´æ—¶JSON: {json_file_path}")
        except Exception as e:
            print(f"[whisper.cpp] æ— æ³•åˆ é™¤ä¸´æ—¶JSON: {e}")

        progress_cb("Completed")
        return srt_content

    except subprocess.CalledProcessError as e:
        stderr_msg = e.stderr if isinstance(e.stderr, str) else str(e.stderr)
        print(f"[whisper.cpp] Error: {stderr_msg}")
        raise RuntimeError(f"whisper.cpp transcription failed: {stderr_msg}")
    except json.JSONDecodeError as e:
        print(f"[whisper.cpp] JSON parsing error: {e}")
        raise RuntimeError(f"Failed to parse whisper.cpp output: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Helper Functions
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _convert_whisper_cpp_to_srt(json_data: Dict[str, Any]) -> str:
    """
    å°†whisper.cppçš„JSONè¾“å‡ºè½¬æ¢ä¸ºSRTå­—å¹•æ ¼å¼

    whisper.cpp JSONæ ¼å¼:
    {
      "transcription": [
        {
          "timestamps": {"from": "00:00:00,320", "to": "00:00:00,370"},
          "offsets": {"from": 320, "to": 370},
          "text": " word"
        },
        ...
      ]
    }
    """
    srt_lines = []
    index = 1

    transcription = json_data.get("transcription", [])

    for segment in transcription:
        # è·³è¿‡ç©ºæ–‡æœ¬å’Œçº¯ç©ºæ ¼
        text = segment.get("text", "").strip()
        if not text:
            continue

        timestamps = segment.get("timestamps", {})
        time_from = timestamps.get("from", "00:00:00,000")
        time_to = timestamps.get("to", "00:00:00,000")

        # SRTæ ¼å¼: åºå·ã€æ—¶é—´æˆ³ã€æ–‡æœ¬ã€ç©ºè¡Œ
        srt_lines.append(f"{index}")
        srt_lines.append(f"{time_from} --> {time_to}")
        srt_lines.append(text)
        srt_lines.append("")  

        index += 1

    return "\n".join(srt_lines)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Compatibility Layer (ä¿æŒä¸fast_wsr.pyæ¥å£ä¸€è‡´)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_multilingual_transcription_params() -> Dict[str, Any]:
    return {
        'language': None,
        'word_timestamps': True,  # whisper.cppé€šè¿‡ -ml 1 å®ç°
        'beam_size': 5,
        'best_of': 5,
        'temperature': [0.0, 0.2],
    }


def get_model():
    use_gpu = get_use_gpu_setting()
    paths = get_whisper_cpp_paths(use_gpu=use_gpu)
    print(f"[whisper.cpp] Model will be loaded on-demand from: {paths['model_dir']}")
    print(f"[whisper.cpp] Binary: {paths['binary']} (CUDA: {'âœ…' if paths['has_cuda'] else 'âŒ'})")
    print(f"[whisper.cpp] GPU mode: {'Enabled' if use_gpu else 'Disabled (CPU-only)'}")

    if use_gpu and not paths['has_cuda']:
        print(f"[whisper.cpp] âš ï¸ Warning: GPU enabled but binary doesn't support CUDA")

    return None 
