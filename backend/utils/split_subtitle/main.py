import os
import re

import difflib
from typing import List, Tuple
import sys
from concurrent.futures import ThreadPoolExecutor, as_completed
from utils.split_subtitle.ASRData import ASRData, from_srt, ASRDataSeg
from utils.split_subtitle.split_by_llm import split_by_llm
from utils.split_subtitle.merge_english_words import WordMerger

MAX_DISPLAY_COUNT = 60  # displayé•¿åº¦çš„æœ€å¤§æ•°é‡
MIN_DISPLAY_COUNT = 10   # displayé•¿åº¦çš„æœ€å°æ•°é‡
SEGMENT_THRESHOLD = 800  # æ¯ä¸ªåˆ†æ®µçš„æœ€å¤§å­—æ•°
FIXED_NUM_THREADS = 4  # å›ºå®šçš„çº¿ç¨‹æ•°é‡
SPLIT_RANGE = 50  # åœ¨åˆ†å‰²ç‚¹å‰åå¯»æ‰¾æœ€å¤§æ—¶é—´é—´éš”çš„èŒƒå›´

import logging
logger = logging.getLogger('subtitle_split')
if not logger.handlers:
    # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    
    # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
    log_file = os.path.join(os.path.dirname(__file__), '..', '..', 'logs', 'subtitle_split.log')
    os.makedirs(os.path.dirname(log_file), exist_ok=True)
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    
    # åˆ›å»ºæ ¼å¼å™¨
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)
    file_handler.setFormatter(formatter)
    
    # æ·»åŠ å¤„ç†å™¨åˆ°logger
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    logger.setLevel(logging.DEBUG)

def is_pure_punctuation(s: str) -> bool:
    """
    æ£€æŸ¥å­—ç¬¦ä¸²æ˜¯å¦ä»…ç”±æ ‡ç‚¹ç¬¦å·ç»„æˆ
    """
    return not re.search(r'\w', s, flags=re.UNICODE)


from utils.split_subtitle.cnt_tokens import count_words, cnt_display_words

def preprocess_text(s: str) -> str:
    """
    é€šè¿‡è½¬æ¢ä¸ºå°å†™å¹¶è§„èŒƒåŒ–ç©ºæ ¼æ¥æ ‡å‡†åŒ–æ–‡æœ¬
    """
    return ' '.join(s.lower().split())


def merge_segments_based_on_sentences(asr_data: ASRData, sentences: List[str]) -> ASRData:
    """
    åŸºäºLLMè¿”å›çš„å¥å­åˆ—è¡¨ï¼Œåˆå¹¶ASRåˆ†æ®µã€‚
    asr_data: ASRDataï¼ŒASRDataSeg Listï¼Œæ®µè½ä¸­æ‰€æœ‰çš„token
    sentences: List[str],LLMæ ¹æ®å®Œæ•´å¥å­ç”Ÿæˆçš„åˆ†å¥åˆ—è¡¨
    è¿™é‡Œå·²ç»ç²—ç­›è¿‡ä¸€æ¬¡ï¼Œasr_dataå’Œsentenceéƒ½æ˜¯è¿™ä¸ª800å­—å·¦å³çš„Section
    """
    asr_texts = [seg.text for seg in asr_data.segments]
    asr_len = len(asr_texts)
    asr_index = 0  # å½“å‰åˆ†æ®µç´¢å¼•ä½ç½®
    threshold = 0.5  # ç›¸ä¼¼åº¦é˜ˆå€¼
    max_shift = 10   # æ»‘åŠ¨çª—å£çš„æœ€å¤§åç§»é‡

    new_segments = []  # å­˜å‚¨List[List[ASRDataSeg]]ä»¥ä¿ç•™tokenæ—¶é—´ä¿¡æ¯

    for sentence in sentences:
        logger.info(f"[+] å¤„ç†å¥å­: {sentence}")
        sentence_proc = preprocess_text(sentence)
        word_count = count_words(sentence_proc)
        best_ratio = 0.0
        best_pos = None
        best_window_size = 0

        # æ»‘åŠ¨çª—å£å¤§å°ï¼Œä¼˜å…ˆè€ƒè™‘æ¥è¿‘å¥å­è¯æ•°çš„çª—å£
        # æœ€å¤šä¸ºç›®æ ‡è¯æ•°çš„2å€
        # ä½†ä¸èƒ½è¶…è¿‡å‰©ä½™çš„ASRæ•°æ®é•¿åº¦
        min_window_size = max(1, word_count // 2)
        # è‡³å°‘ä¸º1ï¼ˆä¿è¯çª—å£ä¸ä¸ºç©ºï¼‰
        # é€šå¸¸ä¸ºç›®æ ‡è¯æ•°çš„ä¸€åŠ
        max_window_size = min(word_count * 2, asr_len - asr_index)

        window_sizes = sorted(range(min_window_size, max_window_size + 1), key=lambda x: abs(x - word_count))

        for window_size in window_sizes:
            max_start = min(asr_index + max_shift + 1, asr_len - window_size + 1)
            for start in range(asr_index, max_start):
                substr = ''.join(asr_texts[start:start + window_size])
                substr_proc = preprocess_text(substr)
                ratio = difflib.SequenceMatcher(None, sentence_proc, substr_proc).ratio()
                if ratio > best_ratio:
                    best_ratio = ratio
                    best_pos = start
                    best_window_size = window_size
                if ratio == 1.0:
                    break  # å®Œå…¨åŒ¹é…
            if best_ratio == 1.0:
                break  # å®Œå…¨åŒ¹é…

        if best_ratio >= threshold and best_pos is not None:
            start_seg_index = best_pos
            end_seg_index = best_pos + best_window_size - 1

            # ä¿ç•™åŸå§‹çš„ASRDataSegåˆ—è¡¨ï¼Œä¿æŒtokençº§åˆ«çš„æ—¶é—´ä¿¡æ¯
            matched_segments = asr_data.segments[start_seg_index:end_seg_index + 1]
            merged_text = ''.join(seg.text for seg in matched_segments)

            print(f"[+] åˆå¹¶åˆ†æ®µ: {merged_text}")
            print("=============")

            # å­˜å‚¨åŒ¹é…çš„åˆ†æ®µåˆ—è¡¨ï¼Œä¿æŒtokenæ—¶é—´ä¿¡æ¯
            new_segments.append(matched_segments)

            asr_index = end_seg_index + 1  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªæœªå¤„ç†çš„åˆ†æ®µ
        else:
            # æ— æ³•åŒ¹é…å¥å­ï¼Œè·³è¿‡å½“å‰åˆ†æ®µ
            print(f"[-] æ— æ³•åŒ¹é…å¥å­: {sentence},å°è¯•åŒ¹é…è¿‡çš„åˆ†æ®µä¸º{substr_proc},è¯æ•°ä¸º{word_count}å…¶SequenceåŒ¹é…åº¦ä¸º{ratio}")
            # åŒ¹é…å¤±è´¥æ—¶åªå‰è¿›1æ­¥ï¼Œè€Œä¸æ˜¯è·³è¿‡æ•´ä¸ªçª—å£
            asr_index += 1

    # ç»Ÿä¸€å¤„ç†è¿‡é•¿å’Œè¿‡çŸ­çš„åˆ†æ®µ
    print("[+] æ­£åœ¨å¤„ç†è¿‡é•¿åˆ†æ®µ...")
    processed_segments = []
    for seg_list in new_segments:
        # è®¡ç®—æ•´ä¸ªå¥å­çš„displayé•¿åº¦
        total_text = ''.join(seg.text for seg in seg_list)
        display_len = cnt_display_words(total_text)
        
        if display_len > MAX_DISPLAY_COUNT:
            # éœ€è¦åˆ†å‰²çš„åˆ†æ®µï¼Œä¼ é€’List[ASRDataSeg]
            split_segs = split_segment_by_display_length(seg_list)
            processed_segments.extend(split_segs)
        else:
            # åˆ›å»ºå•ä¸ªåˆå¹¶çš„ASRDataSegç”¨äºæœ€ç»ˆè¾“å‡º
            merged_seg = ASRDataSeg(
                total_text,
                seg_list[0].start_time,
                seg_list[-1].end_time
            )
            processed_segments.append(merged_seg)
    
    print("[+] æ­£åœ¨å¾ªç¯åˆå¹¶è¿‡çŸ­åˆ†æ®µ...")
    final_segments = merge_short_segments_iteratively(processed_segments)
    
    # å¯¹æœ€ç»ˆçš„å¥å­æ–‡æœ¬åº”ç”¨è‹±æ–‡å•è¯åˆå¹¶
    print("[+] æ­£åœ¨åˆå¹¶åˆ†å‰²çš„è‹±æ–‡å•è¯...")
    word_merger = WordMerger()
    merged_final_segments = []
    
    for seg in final_segments:
        merged_text = word_merger.merge_text(seg.text)
        if merged_text != seg.text:
            print(f"[+] åˆå¹¶å•è¯: '{seg.text}' -> '{merged_text}'")
        
        # åˆ›å»ºæ–°çš„segmentï¼Œä¿æŒæ—¶é—´ä¿¡æ¯ä¸å˜
        merged_seg = ASRDataSeg(
            merged_text,
            seg.start_time,
            seg.end_time,
            seg.direct,
            seg.free,
            seg.reflected
        )
        merged_final_segments.append(merged_seg)
    
    return ASRData(merged_final_segments)


def split_segment_by_display_length(seg_list: List[ASRDataSeg]) -> List[ASRDataSeg]:
    """
    åŸºäºdisplayé•¿åº¦åˆ†å‰²ASRDataSegåˆ—è¡¨ï¼Œä¿æŒtokençº§åˆ«çš„æ—¶é—´ä¿¡æ¯
    """
    result = []
    
    if not seg_list:
        return result
    
    # è®¡ç®—æ€»çš„displayé•¿åº¦
    total_text = ''.join(seg.text for seg in seg_list)
    total_display_len = cnt_display_words(total_text)
    
    if total_display_len <= MAX_DISPLAY_COUNT:
        # ä¸éœ€è¦åˆ†å‰²ï¼Œåˆå¹¶ä¸ºå•ä¸ªsegment
        merged_seg = ASRDataSeg(
            total_text,
            seg_list[0].start_time,
            seg_list[-1].end_time
        )
        result.append(merged_seg)
        return result
    
    # éœ€è¦åˆ†å‰²ï¼Œä½¿ç”¨æ—¶é—´é—´éš”å¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
    n = len(seg_list)
    if n <= 1:
        # åªæœ‰ä¸€ä¸ªsegmentï¼Œç›´æ¥è¿”å›
        result.extend(seg_list)
        return result
    logger.info(f"[+] å¼€å§‹åˆ†å‰²ASRDataSegåˆ—è¡¨ï¼Œé•¿åº¦ä¸º {n}")
    
    # åœ¨1/6åˆ°5/6ä¹‹é—´å¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
    start_idx = max(1, n // 6)
    end_idx = min(n - 1, (5 * n) // 6)
    
    max_time_diff = 0
    best_split_idx = -1
    
    for i in range(start_idx, end_idx + 1):
        # è®¡ç®—ä¸å‰ä¸€ä¸ªtokençš„æ—¶é—´é—´éš”
        time_diff = seg_list[i].start_time - seg_list[i - 1].end_time
        if time_diff > max_time_diff:
            max_time_diff = time_diff
            best_split_idx = i
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹æˆ–æ—¶é—´å·®å¤ªå°ï¼Œä½¿ç”¨ä¸­ç‚¹
    if best_split_idx == -1 or max_time_diff < 50:  # 50æ¯«ç§’ä½œä¸ºé˜ˆå€¼
        best_split_idx = n // 2
    
    # æ ¹æ®æ‰¾åˆ°çš„åˆ†å‰²ç‚¹åˆ†å‰²
    first_part = seg_list[:best_split_idx]
    second_part = seg_list[best_split_idx:]
    
    # é€’å½’å¤„ç†ä¸¤ä¸ªéƒ¨åˆ†
    result.extend(split_segment_by_display_length(first_part))
    result.extend(split_segment_by_display_length(second_part))
    
    return result

def merge_short_segments_iteratively(segments: List[ASRDataSeg]) -> List[ASRDataSeg]:
    """
    å¾ªç¯åˆå¹¶è¿‡çŸ­çš„åˆ†æ®µï¼ŒåŸºäºdisplay_wordsæœ€å°å€¼
    """
    result = segments.copy()
    changed = True
    
    while changed:
        changed = False
        i = 0
        
        while i < len(result):
            seg = result[i]
            display_len = cnt_display_words(seg.text)
            
            if display_len < MIN_DISPLAY_COUNT:
                # è®¡ç®—ä¸å‰ååˆ†æ®µçš„æ—¶é—´å·®
                prev_time_diff = float('inf')
                next_time_diff = float('inf')
                
                if i > 0:
                    prev_seg = result[i - 1]
                    prev_time_diff = seg.start_time - prev_seg.end_time
                
                if i < len(result) - 1:
                    next_seg = result[i + 1]
                    next_time_diff = next_seg.start_time - seg.end_time
                
                # é€‰æ‹©æ—¶é—´å·®è¾ƒå°çš„è¿›è¡Œåˆå¹¶
                if prev_time_diff <= next_time_diff and i > 0:
                    # ä¸å‰ä¸€ä¸ªåˆ†æ®µåˆå¹¶
                    prev_seg = result[i - 1]
                    merged_text = prev_seg.text + seg.text
                    
                    merged_seg = ASRDataSeg(
                        merged_text,
                        prev_seg.start_time,
                        seg.end_time,
                        "",  # ç›´æ¥ç¿»è¯‘
                        "",  # è‡ªç”±ç¿»è¯‘
                        ""   # åæ€ç¿»è¯‘
                    )
                    result[i - 1:i + 1] = [merged_seg]
                    changed = True
                    # ä¸å¢åŠ iï¼Œå› ä¸ºåˆ—è¡¨é•¿åº¦å‡å°‘äº†
                elif next_time_diff < prev_time_diff and i < len(result) - 1:
                    # ä¸åä¸€ä¸ªåˆ†æ®µåˆå¹¶
                    next_seg = result[i + 1]
                    merged_text = seg.text + next_seg.text
                    
                    merged_seg = ASRDataSeg(
                        merged_text,
                        seg.start_time,
                        next_seg.end_time,
                        "",  # ç›´æ¥ç¿»è¯‘
                        "",  # è‡ªç”±ç¿»è¯‘
                        ""   # åæ€ç¿»è¯‘
                    )
                    result[i:i + 2] = [merged_seg]
                    changed = True
                    # ä¸å¢åŠ iï¼Œç»§ç»­æ£€æŸ¥åˆå¹¶åçš„åˆ†æ®µ
                else:
                    # æ— æ³•åˆå¹¶ï¼ˆè¾¹ç•Œæƒ…å†µï¼‰
                    i += 1
            else:
                i += 1
    
    return result


def process_split_by_llm(asr_data_part: ASRData) -> List[str]:
    """
    è°ƒç”¨ split_by_llm å¤„ç†å•ä¸ª ASRData åˆ†æ®µï¼Œè¿”å›å¥å­åˆ—è¡¨
    """
    txt = asr_data_part.to_txt().replace("\n", "")
    sentences = split_by_llm(txt, use_cache=True)
    return sentences


def split_asr_data(asr_data: ASRData, num_segments: int) -> List[ASRData]:
    """
    æ ¹æ®ASRåˆ†æ®µä¸­çš„æ—¶é—´é—´éš”ï¼Œå°†ASRDataæ‹†åˆ†æˆå¤šä¸ªéƒ¨åˆ†ã€‚
    å¤„ç†æ­¥éª¤ï¼š
    1. è®¡ç®—æ€»å­—æ•°ä¸åˆ†æ®µæ•°ï¼Œå¹¶ç¡®å®šæ¯ä¸ªåˆ†æ®µçš„å­—æ•°èŒƒå›´ã€‚
    2. ç¡®å®šå¹³å‡åˆ†å‰²ç‚¹ã€‚
    3. åœ¨åˆ†å‰²ç‚¹å‰åä¸€å®šèŒƒå›´å†…ï¼Œå¯»æ‰¾æ—¶é—´é—´éš”æœ€å¤§çš„ç‚¹ä½œä¸ºå®é™…çš„åˆ†å‰²ç‚¹ã€‚
    è¿”å›ï¼šList[ASRData]ï¼Œå³åˆ†æ®µåˆ—è¡¨ï¼Œæ¯ä¸€ä¸ªASRDataå¯¹è±¡ä»£è¡¨ä¸€ä¸ªåˆ†æ®µ,åŒ…å«å…¶ä¸­æ‰€æœ‰çš„ASRDataSegå¯¹è±¡ã€‚
    """
    total_segs = len(asr_data.segments)
    total_word_count = count_words(asr_data.to_txt())
    words_per_segment = total_word_count // num_segments
    split_indices = []

    if num_segments <= 1 or total_segs <= num_segments:
        return [asr_data]

    # è®¡ç®—æ¯ä¸ªåˆ†æ®µçš„å¤§è‡´å­—æ•° æ ¹æ®æ¯æ®µå­—æ•°è®¡ç®—åˆ†å‰²ç‚¹
    # æ¯”å¦‚7047ä¸ªå­—ï¼Œåˆ†æˆ8æ®µï¼Œæ¯æ®µ880ä¸ªå­—ã€‚
    split_indices = [i * words_per_segment for i in range(1, num_segments)]
    # è°ƒæ•´åˆ†å‰²ç‚¹ï¼šåœ¨æ¯ä¸ªå¹³å‡åˆ†å‰²ç‚¹é™„è¿‘å¯»æ‰¾æ—¶é—´é—´éš”æœ€å¤§çš„ç‚¹
    adjusted_split_indices = []
    for split_point in split_indices:
        # å®šä¹‰æœç´¢èŒƒå›´
        start = max(0, split_point - SPLIT_RANGE)
        end = min(total_segs - 1, split_point + SPLIT_RANGE)
        # åœ¨èŒƒå›´å†…æ‰¾åˆ°æ—¶é—´é—´éš”æœ€å¤§çš„ç‚¹
        max_gap = -1
        best_index = split_point
        for j in range(start, end):
            gap = asr_data.segments[j + 1].start_time - asr_data.segments[j].end_time
            if gap > max_gap:
                max_gap = gap
                best_index = j
        adjusted_split_indices.append(best_index)

    # ç§»é™¤é‡å¤çš„åˆ†å‰²ç‚¹
    adjusted_split_indices = sorted(list(set(adjusted_split_indices)))

    # æ ¹æ®è°ƒæ•´åçš„åˆ†å‰²ç‚¹æ‹†åˆ†ASRData
    segments = []
    prev_index = 0
    for index in adjusted_split_indices:
        part = ASRData(asr_data.segments[prev_index:index + 1])
        segments.append(part)
        prev_index = index + 1
    # æ·»åŠ æœ€åä¸€éƒ¨åˆ†
    if prev_index < total_segs:
        part = ASRData(asr_data.segments[prev_index:])
        segments.append(part)

    return segments


def determine_num_segments(word_count: int, threshold: int = 500) -> int:
    """
    æ ¹æ®å­—æ•°è®¡ç®—åˆ†æ®µæ•°ï¼Œæ¯1000ä¸ªå­—ä¸ºä¸€ä¸ªåˆ†æ®µï¼Œè‡³å°‘ä¸º1
    """
    num_segments = word_count // threshold
    # å¦‚æœå­˜åœ¨ä½™æ•°ï¼Œå¢åŠ ä¸€ä¸ªåˆ†æ®µ
    if word_count % threshold > 0:
        num_segments += 1
    return max(1, num_segments)

from video.views.set_setting import load_all_settings

from typing import Callable, List
def optimise_srt(
    srt_path: str,
    save_path: str,
    num_threads: int = FIXED_NUM_THREADS,
    progress_cb: Callable[[float], None] | None = None,   # 0.0â€’1.0 ä¹‹é—´
) -> None:
    settings = load_all_settings()
    use_proxy = settings.get('DEFAULT', {}).get('use_proxy', 'true').lower() == 'true'
    if not use_proxy:
        # ç¦ç”¨HTTP(S)ä»£ç†è¯·æ±‚
        os.environ.pop('http_proxy', None)
        os.environ.pop('https_proxy', None)
    from utils.llm_engines import ENGINES
    # è·å– API é…ç½®
    selected_model_provider = settings.get('DEFAULT', {}).get('selected_model_provider', 'deepseek')
    api_key = settings.get('DEFAULT', {}).get(f'{selected_model_provider}_api_key', '')
    base_url = settings.get('DEFAULT', {}).get(f'{selected_model_provider}_base_url', 'https://api.deepseek.com')
    enable_thinking = settings.get('DEFAULT', {}).get('enable_thinking', 'true')
    model = ENGINES[selected_model_provider]["thinking" if enable_thinking == 'true' else "normal"]
    """
    Â· å°† ç”¨äºä¼˜åŒ–å­—å¹•ã€‚
    Â· å¢åŠ  progress_cb å›è°ƒï¼Œç”¨äºä¸ŠæŠ¥é˜¶æ®µå†…è¿›åº¦ï¼ˆ0â€‘1ï¼‰
      â€‘ è‹¥æœªä¼ é€’åˆ™é»˜è®¤ä»€ä¹ˆéƒ½ä¸åš
    """
    if progress_cb is None:               # å›è°ƒé»˜è®¤ç©ºæ“ä½œ
        progress_cb = lambda ratio: None

    if progress_cb:
        progress_cb("Running")      # è¯»å–srtæ–‡ä»¶ä¸ºä¸€æ•´ä¸ªasr_data
    with open(srt_path, encoding="utf-8") as f:
        asr_data = from_srt(f.read())

    # é¢„å¤„ç†ASRæ•°æ®ï¼Œå»é™¤æ ‡ç‚¹å¹¶è½¬æ¢ä¸ºå°å†™
    new_segments= []
    for seg in asr_data.segments:
        if not is_pure_punctuation(seg.text):
            if re.match(r"^[a-zA-Z\']+$", seg.text.strip()):
                seg.text = seg.text.lower() + " "
            new_segments.append(seg)
    asr_data.segments = new_segments

    # å°†æ•´ä¸ªå­—å¹•åˆå¹¶ä¸ºæ–‡æœ¬
    txt = asr_data.to_txt().replace("\n", "")
    total_word_count = count_words(txt)
    print(f"[+] å¾…åˆ†å‰²æ®µè½æ–‡æœ¬é•¿åº¦: {total_word_count} å­—")

    # è°ƒè¯•ï¼šæ‰“å°åˆå¹¶åçš„æ–‡æœ¬å‰500å­—ç¬¦
    logger.debug(f"[DEBUG] å¾…åˆ†å‰²æ–‡æœ¬å‰500å­—ç¬¦: {txt[:500]}")

    # æ‰“å°æ‰€æœ‰word-levelæ—¶é—´æˆ³å­—å¹•ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    logger.debug("[DEBUG] ========== Word-level Timestamps ==========")
    logger.debug(f"[DEBUG] æ€»å…± {len(asr_data.segments)} ä¸ªword segments")
    for i, seg in enumerate(asr_data.segments[:20]):  # åªæ‰“å°å‰20ä¸ª
        # start_timeå’Œend_timeæ˜¯æ¯«ç§’ï¼Œéœ€è¦é™¤ä»¥1000è½¬æ¢ä¸ºç§’
        logger.debug(f"[DEBUG] Segment {i}: [{seg.start_time/1000:.2f}s - {seg.end_time/1000:.2f}s] '{seg.text}'")
    if len(asr_data.segments) > 20:
        logger.debug(f"[DEBUG] ... (è¿˜æœ‰ {len(asr_data.segments) - 20} ä¸ªsegments)")
    logger.debug("[DEBUG] ============================================")

    num_segments = determine_num_segments(
        total_word_count, threshold=SEGMENT_THRESHOLD
    )
    print(f"[+] æ ¹æ®å­—æ•° {total_word_count}ï¼Œç¡®å®šåˆ†æ®µæ•°: {num_segments}")

    # åˆ†å‰²ASRData
    asr_data_segments = split_asr_data(asr_data, num_segments)

    # â”€â”€ 10â€‘85â€¯%ï¼š å¤šçº¿ç¨‹æ‰§è¡Œ split_by_llm è·å–å¥å­åˆ—è¡¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ğŸ†• è¿›åº¦è¿½è¸ªå˜é‡
    import threading
    completed_chunks = 0
    total_chunks = len(asr_data_segments)
    progress_lock = threading.Lock()

    def process_segment(asr_data_part):
        nonlocal completed_chunks
        part_txt = asr_data_part.to_txt().replace("\n", "")
        sentences = split_by_llm(part_txt, use_cache=True,api_key=api_key,model=model,base_url=base_url)
        print(f"[+] åˆ†æ®µçš„å¥å­æå–å®Œæˆï¼Œå…± {len(sentences)} å¥")
        # ğŸ†• çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°è¿›åº¦ (10% ~ 85%)
        with progress_lock:
            completed_chunks += 1
            progress_percent = 10 + int((completed_chunks / total_chunks) * 75)
            if progress_cb:
                progress_cb(progress_percent)
        return sentences
    print("[+] æ­£åœ¨å¹¶è¡Œè¯·æ±‚LLMå°†æ¯ä¸ªåˆ†æ®µçš„æ–‡æœ¬æ‹†åˆ†ä¸ºå¥å­...")
    all_sentences: List[str] = [] # ä¸€ä¸ªäºŒç»´åˆ—è¡¨
    """
    â€¢ map â‡’ é¡ºåºå›ºå®šï¼Œé€‚åˆâ€œä¸€æ¬¡æ€§æ‹¿é½â€åœºæ™¯ã€‚
    â€¢ submit + as_completed â‡’ å®Œæˆé¡ºåºéšæœºï¼Œå¦‚éœ€ä¿æŒåŸåºå¿…é¡»è‡ªå·±æ ¹æ®ç´¢å¼•æ’é˜Ÿã€‚
    â€¢ ä½ çœ‹åˆ°çš„å¤§é‡â€œæ— æ³•åŒ¹é…â€å¹¶éçº¿ç¨‹æ± é€Ÿåº¦é—®é¢˜ï¼Œè€Œæ˜¯ç»“æœé¡ºåºè¢«æ‰“ä¹±å¯¼è‡´åç»­ç®—æ³•å¯¹ä¸äº†å·ã€‚
    â€¢ æ—¢ç„¶ map å·²ç»æ»¡è¶³æ€§èƒ½è¦æ±‚ï¼Œåˆå¤©ç„¶ä¿è¯é¡ºåºï¼Œæœ€ç®€å•çš„å°±æ˜¯ä¿ç•™ map å†™æ³•ã€‚(æ‰€ä»¥è¿™é‡Œåªéœ€è¦æä¾›æ˜¯å¦å®Œæˆï¼Œä¸éœ€è¦æä¾›è¿›åº¦)
    """
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        all_sentences = list(executor.map(process_segment, asr_data_segments))
    logger.info("all_sentences before flatten: %s", all_sentences)
    all_sentences = [item for sublist in all_sentences for item in sublist] # æ‘Šå¹³å…ƒç´ ï¼Œall_sentences è¢«å‡å®šä¸ºäºŒç»´åˆ—è¡¨
    logger.info("all_sentences after flatten: %s", all_sentences)

    """
    ç¤ºä¾‹ï¼š
    all_sentences = [
        ['I', 'am', 'AI'],
        ['ä½ ', 'å¥½'],
        ['hello', 'world']
    ]
    # æ‰§è¡Œåˆ—è¡¨æ¨å¯¼å¼åï¼š
    all_sentences
    # -> ['I', 'am', 'AI', 'ä½ ', 'å¥½', 'hello', 'world']
    """

    print(f"[+] æ€»å…±æå–åˆ° {len(all_sentences)} å¥")

    # åŸºäºLLMå·²ç»åˆ†æ®µçš„å¥å­ï¼Œå¯¹ASRåˆ†æ®µè¿›è¡Œåˆå¹¶
    print("[+] æ­£åœ¨åˆå¹¶ASRåˆ†æ®µåŸºäºå¥å­åˆ—è¡¨...")
    # â”€â”€ 85â€‘95â€¯%ï¼šåˆå¹¶åˆ†æ®µ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    merged_asr = merge_segments_based_on_sentences(asr_data, all_sentences)
    merged_asr.segments.sort(key=lambda s: s.start_time)

    # â”€â”€ 95â€‘100â€¯%ï¼šå†™æ–‡ä»¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    final_asr_data = ASRData(merged_asr.segments)
    final_asr_data.to_srt(save_path=save_path,use_translation=False)
    print("[+] å·²å®Œæˆ srt æ–‡ä»¶åˆå¹¶")
    if progress_cb:
        progress_cb("Completed")        

    print("[+] å·²å®Œæˆ srt æ–‡ä»¶åˆå¹¶")

def translate_srt(raw_srt_path, 
                  translate_srt_path,
                  raw_lang="en", 
                  target_lang="zh",
                  use_translation_cache: bool = True,  # Whether to use translation cache
                  num_threads: int = FIXED_NUM_THREADS,  # Number of threads for translation
                  batch_size: int = 15,  # Batch size for LLM processing (10-20 sentences per batch)
                  progress_cb: Callable[[float], None] | None = None,
                  terms_to_note: str = "",  # Terms to emphasize in translation
    ):
    """
    ç¿»è¯‘ SRT æ–‡ä»¶çš„ä¸»å‡½æ•°
    """
    import logging
    from utils.split_subtitle.translate import two_step_translate
    
    logger = logging.getLogger('subtitle_translate')
    logger.info("å¼€å§‹ç¿»è¯‘å­—å¹•...")
    
    # ä»raw_srt_pathåŠ è½½åŸå§‹å­—å¹•
    with open(raw_srt_path, encoding="utf-8") as f:
        raw_asr_data = from_srt(f.read())
    logger.info("åŸæ–‡å­—å¹•åŠ è½½å®Œæˆ")
    
    # ç¿»è¯‘å­—å¹•
    final_asr_data = two_step_translate(raw_asr_data, use_cache=use_translation_cache, num_threads=num_threads, batch_size=batch_size, source_lang=raw_lang, target_lang=target_lang, terms_to_note=terms_to_note)
    logger.info("å­—å¹•ç¿»è¯‘å®Œæˆ")
    
    # å¦‚æœæä¾›äº†ç¿»è¯‘ä¿å­˜è·¯å¾„ï¼Œåˆ™ä¿å­˜ç¿»è¯‘å­—å¹•
    if translate_srt_path:
        final_asr_data.to_srt(save_path=translate_srt_path, use_translation=True)
        logger.info(f"ä¿å­˜ç¿»è¯‘å­—å¹•: {translate_srt_path}")
    else:
        logger.warning("æœªæä¾›ç¿»è¯‘å­—å¹•ä¿å­˜è·¯å¾„ï¼Œç¿»è¯‘å­—å¹•æœªä¿å­˜")
    
    logger.info("ç¿»è¯‘å®Œæˆ")
    if progress_cb:
        progress_cb("Completed")

# 'çº¿ç¨‹æ•°é‡'
num_threads=FIXED_NUM_THREADS
